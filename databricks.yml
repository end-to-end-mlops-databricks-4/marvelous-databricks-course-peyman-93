# This is a Databricks asset bundle definition for marvelous-databricks-course-peyman-93.
# The Databricks extension requires databricks.yml configuration file.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.

bundle:
  name: marvelous-databricks-course-peyman-93

include:
  - "project_config.yml"

sync:
  include:
    - "data/*.csv"

artifacts:
  default:
    type: whl
    build: uv build
    path: .

variables:
  git_sha:
    description: Git commit SHA
    default: abcd
  repo:
    description: Repository name
    default: marvelous-databricks-course-peyman-93
  org:
    description: GitHub organization or user
    default: end-to-end-mlops-databricks-4
  branch:
    description: Git branch
    default: main
  schedule_pause_status:
    description: Schedule pause status (PAUSED or UNPAUSED)
    default: PAUSED
  is_test:
    description: Test mode flag (0 for production, 1 for test)
    default: 0
  env:
    description: Environment (dev, acc, or prd)
    default: dev

resources:
  jobs:
    deployment:
      name: ${bundle.name}-workflow
      # Uncomment to add email notifications on job failure
      # email_notifications:
      #   on_failure:
      #     - your-email@example.com
      schedule:
        quartz_cron_expression: "0 0 6 ? * MON"  # Run every Monday at 6 AM
        timezone_id: "Europe/Amsterdam"
        pause_status: ${var.schedule_pause_status}
      tags:
        project_name: "financial-complaints"
      job_clusters:
        - job_cluster_key: "complaints-cluster"
          new_cluster:
            spark_version: "16.4.x-scala2.12"
            data_security_mode: "SINGLE_USER"
            node_type_id: "r3.xlarge"
            driver_node_type_id: "r3.xlarge"
            autoscale:
              min_workers: 1
              max_workers: 2
            spark_env_vars:
              "TOKEN_STATUS_CHECK": "{{secrets/mlops_course/git_token_status_check}}"

      tasks:
        # Task 1: Data Processing
        - task_key: "preprocessing"
          # Use existing cluster
          existing_cluster_id: "1016-224704-pj1xopz7"
          spark_python_task:
            python_file: "scripts/01.process_data.py"
            parameters:
              - "--root_path"
              - "${workspace.root_path}"
              - "--env"
              - "${var.env}"
              - "--is_test"
              - "${var.is_test}"
          libraries:
            - whl: ./dist/*.whl

        # Task 2: Model Training with Feature Engineering
        - task_key: "train_model"
          existing_cluster_id: "1016-224704-pj1xopz7"
          depends_on:
            - task_key: "preprocessing"
          spark_python_task:
            python_file: "scripts/02.train_register_fe_model.py"
            parameters:
              - "--root_path"
              - "${workspace.root_path}"
              - "--env"
              - "${var.env}"
              - "--git_sha"
              - "${var.git_sha}"
              - "--job_run_id"
              - "{{job.run_id}}"
              - "--branch"
              - "${var.branch}"
              - "--is_test"
              - "${var.is_test}"
          libraries:
            - whl: ./dist/*.whl

        # Condition Task: Check if Model Was Updated
        - task_key: "model_updated"
          condition_task:
            op: "EQUAL_TO"
            left: "{{tasks.train_model.values.model_updated}}"
            right: "1"
          depends_on:
            - task_key: "train_model"

        # Task 3: Model Deployment (only if model was updated)
        - task_key: "deploy_model"
          depends_on:
            - task_key: "model_updated"
              outcome: "true"
          existing_cluster_id: "1016-224704-pj1xopz7"
          spark_python_task:
            python_file: "scripts/03.deploy_model.py"
            parameters:
              - "--root_path"
              - "${workspace.root_path}"
              - "--env"
              - "${var.env}"
              - "--is_test"
              - "${var.is_test}"
          libraries:
            - whl: ./dist/*.whl

        # Condition Task: Check if GitHub Status Post is Required (test mode only)
        - task_key: "post_commit_status_required"
          condition_task:
            op: "EQUAL_TO"
            left: "${var.is_test}"
            right: "1"
          depends_on:
            - task_key: "deploy_model"

        # Task 4: Post Commit Status to GitHub (only in test mode)
        - task_key: "post_commit_status"
          depends_on:
            - task_key: "post_commit_status_required"
              outcome: "true"
          existing_cluster_id: "1016-224704-pj1xopz7"
          spark_python_task:
            python_file: "scripts/04.post_commit_status.py"
            parameters:
              - "success"  # Status: success, failure, pending, or error
              - "--job_run_id"
              - "{{job.run_id}}"
              - "--job_id"
              - "{{job.id}}"
              - "--git_sha"
              - "${var.git_sha}"
              - "--repo"
              - "${var.repo}"
              - "--org"
              - "${var.org}"
          libraries:
            - whl: ./dist/*.whl

targets:
  # Development Target
  dev:
    default: true
    mode: development
    workspace:
      host: https://dbc-f122dc18-1b68.cloud.databricks.com
      root_path: /Workspace/Users/${workspace.current_user.userName}/.bundle/${bundle.target}/${bundle.name}
    variables:
      schedule_pause_status: PAUSED
      is_test: 0
      env: dev
    artifacts:
      default:
        type: whl
        build: uv build
        path: .
        dynamic_version: true

  # Test Target (for integration testing)
  test:
    presets:
      name_prefix: 'test_'
    workspace:
      host: https://dbc-f122dc18-1b68.cloud.databricks.com
      root_path: /Shared/.bundle/${bundle.target}/${bundle.name}
    variables:
      schedule_pause_status: PAUSED
      is_test: 1
      env: dev
    artifacts:
      default:
        type: whl
        build: uv build
        path: .
        dynamic_version: true

  # Acceptance Target (pre-production validation)
  acc:
    presets:
      name_prefix: 'acc_'
    workspace:
      host: https://dbc-f122dc18-1b68.cloud.databricks.com
      root_path: /Shared/.bundle/${bundle.target}/${bundle.name}
    variables:
      schedule_pause_status: PAUSED
      is_test: 0
      env: acc
    artifacts:
      default:
        type: whl
        build: uv build
        path: .
        dynamic_version: true

  # Production Target
  prd:
    mode: production
    workspace:
      host: https://dbc-f122dc18-1b68.cloud.databricks.com
      root_path: /Shared/.bundle/${bundle.target}/${bundle.name}
    variables:
      schedule_pause_status: PAUSED  # Change to UNPAUSED to enable scheduled runs
      is_test: 0
      env: prd
    artifacts:
      default:
        type: whl
        build: uv build
        path: .
        dynamic_version: false
